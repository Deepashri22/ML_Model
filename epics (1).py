# -*- coding: utf-8 -*-
"""epics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eIuZJoju-3yXB9togtYEoF9wDEP9nHdV

# ---------------------------------------------

### New Code
"""

# Commented out IPython magic to ensure Python compatibility.
#Import necessary modules
import numpy as np
import pylab as pl
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
from sklearn.utils import shuffle
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.model_selection import cross_val_score, GridSearchCV

content = """Date
Location_ISO_Code
Location
New_Cases
New_Deaths
New_Recovered
New_Active_Cases
Total_Cases
Total_Deaths
Total_Recovered
Total_Active_Cases
Location_Level
City_or_Regency
Province
Country
Continent
Island
Time_Zone
Special_Status
Total_Regencies
Total_Cities
Total_Districts
Total_Urban_Villages
Total_Rural_Villages
Area_(km2)
Population
Population_Density
Longitude
Latitude
New_Cases_per_Million
Total_Cases_per_Million
New_Deaths_per_Million
Total_Deaths_per_Million
Case_Fatality_Rate
Case_Recovered_Rate
Growth_Factor_of_New_Cases
Growth_Factor_of_New_Deaths"""
columns_list = content.split("\n")

# columns_list = [
#     "Date", "Location_ISO_Code", "Location", "New_Cases", "New_Deaths",
#     "New_Recovered", "New_Active_Cases", "Total_Cases", "Total_Deaths",
#     "Total_Recovered", "Total_Active_Cases", "Location_Level",
#     "City_or_Regency", "Province", "Country", "Continent", "Island",
#     "Time_Zone", "Special_Status", "Total_Regencies", "Total_Cities",
#     "Total_Districts", "Total_Urban_Villages", "Total_Rural_Villages",
#     "Area_(km2)", "Population", "Population_Density", "Longitude", "Latitude",
#     "New_Cases_per_Million", "Total_Cases_per_Million", "New_Deaths_per_Million",
#     "Total_Deaths_per_Million", "Case_Fatality_Rate", "Case_Recovered_Rate",
#     "Growth_Factor_of_New_Cases", "Growth_Factor_of_New_Deaths"
# ]

# data1 = pd.read_csv("/content/covid_19_indonesia_time_series_all.csv", header=0, names=columns_list, usecols=range(len(columns_list)))
# data1 = data1.set_index('Location')
# data1.head()

# try:
#     data1 = pd.read_csv("/content/covid_19_indonesia_time_series_all.csv", header=0, names=columns_list, index_col=False)
#     data1 = data1.set_index('Location')
#     data1.head()
# except pd.errors.ParserError as e:
#     # Get the line number where the error occurred
#     error_line = int(str(e).split(' ')[-1][:-1])

#     # Print the problematic line
#     with open("/content/covid_19_indonesia_time_series_all.csv", 'r') as file:
#         for i, line in enumerate(file):
#             if i == error_line - 1:  # Adjust index since Python starts counting from 0
#                 print(f"Problematic Line {error_line}: {line}")
#                 break

data1 = pd.read_csv("/content/covid_19_indonesia_time_series_all.csv",header=0,names = columns_list,index_col=False)
data1 = data1.set_index('Location')
data1.head()

data1=data1.replace([np.inf,-np.inf],np.nan)
data1 = data1.fillna(0)
data1

##Convert sting to numeric
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
def FunLabelEncoder(df):
    for c in df.columns:
        if df.dtypes[c] == object:
            le.fit(df[c].astype(str))
            df[c] = le.transform(df[c].astype(str))
    return df

data1 = FunLabelEncoder(data1)
# Convert float64 columns to integers
data1['New_Cases'] = data1['New_Cases'].astype(int)
data1['New_Deaths'] = data1['New_Deaths'].astype(int)
data1['New_Recovered'] = data1['New_Recovered'].astype(int)

# Display the DataFrame after conversion
print(data1.head())
data1.info()
data1.iloc[235:300,:]

from sklearn.model_selection import train_test_split
Y = data1['New_Cases']
X = data1.drop(columns=['New_Cases'])
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=9)

print('X train shape: ', X_train.shape)
print('Y train shape: ', Y_train.shape)
print('X test shape: ', X_test.shape)
print('Y test shape: ', Y_test.shape)

from sklearn.tree import DecisionTreeRegressor

# We define the model
dtcla = DecisionTreeRegressor(random_state=None)

# We train model
dtcla.fit(X_train, Y_train)

# We predict target values
Y_predict = dtcla.predict(X_test)

from sklearn.model_selection import train_test_split
Y1 = data1['New_Deaths']
X1 = data1.drop(columns=['New_Deaths'])
X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1, Y1, test_size=0.1, random_state=9)

print('X1 train shape: ', X1_train.shape)
print('Y1 train shape: ', Y1_train.shape)
print('X1 test shape: ', X1_test.shape)
print('Y1 test shape: ', Y1_test.shape)

from sklearn.tree import DecisionTreeRegressor

# We define the model
dtcla1 = DecisionTreeRegressor(random_state=None)

# We train model
dtcla1.fit(X1_train, Y1_train)

# We predict target values
Y1_predict = dtcla1.predict(X1_test)

X1_test

submission = pd.DataFrame({'New_Cases':Y_predict,'New_Deaths':Y1_predict})


#Visualize the first 100 rows
submission.head(100)

from sklearn.metrics import r2_score, mean_squared_error

# Calculate R2 score
r2 = r2_score(Y1_test, Y1_predict)

# Calculate mean squared error
mse = mean_squared_error(Y_test, Y_predict)

print("R2 Score:", r2)
print("Mean Squared Error:", mse)

import pandas as pd

# Provided dataset as a dictionary
data3 = {
    "Date": "12-25-2020",
    "Location_ISO_Code": "ID-JK",
    "New_Cases": 190,
    "New_Deaths": 95,
    "New_Recovered": 305,
    "New_Active_Cases": 3358,
    "Total_Cases": 19087,
    "Total_Deaths": 3986,
    "Total_Recovered": 4350,
    "Total_Active_Cases": -20,
    "Location_Level": "Province",
    "City_or_Regency": "",
    "Province": "DKI Jakarta",
    "Country": "Indonesia",
    "Continent": "Asia",
    "Island": "Jawa",
    "Time_Zone": "UTC+07:00",
    "Special_Status": "Daerah Khusus Ibu Kota",
    "Total_Regencies": 1,
    "Total_Cities": 5,
    "Total_Districts": 244,
    "Total_Urban_Villages": 2267,
    "Total_Rural_Villages": 4225,
    "Area_(km2)": 664,
    "Population": 10846145,
    "Population_Density": 26374.31,
    "Longitude": 106.8361183,
    "Latitude": -6.204698991,
    "New_Cases_per_Million": 0.18,
    "Total_Cases_per_Million": 67.6,
    "New_Deaths_per_Million": 98,
    "Total_Deaths_per_Million": 6.84,
    "Case_Fatality_Rate": "53.28%",
    "Case_Recovered_Rate": "100.00%",
    "Growth_Factor_of_New_Cases": "",
    "Growth_Factor_of_New_Deaths": "",
}

# Convert dictionary to DataFrame
df = pd.DataFrame(data3, index=[0])

# Display the DataFrame
df

df1 = FunLabelEncoder(df)
df1.info()
df1.iloc[235:300,:]

X_new_cases = df1.drop(columns=['New_Cases'])
X_new_deaths = df1.drop(columns=['New_Deaths'])
new_cases_predicted = dtcla.predict(X_new_cases)
new_deaths_predicted = dtcla1.predict(X_new_deaths)

predictions_df = pd.DataFrame({'New_Cases': new_cases_predicted, 'New_Deaths': new_deaths_predicted})

# Visualize the predictions
print(predictions_df)

"""### Predicting for the next 10 days starting from 6/11/2020"""

import pandas as pd
import numpy as np

# Define the date range for the next 10 days starting from 6/11/2020
date_range = pd.date_range(start='6/11/2020', periods=10, freq='D')

# Generate random values for each feature based on your requirements
# For simplicity, we'll provide some random values within reasonable ranges

# Date
dates = date_range

# Location ISO Code
location_iso_codes = np.random.choice(['ID-JK', 'ID-SA', 'ID-RI', 'ID-SU'], size=10)

# Location
locations = np.random.choice(['DKI Jakarta', 'Sulawesi Utara', 'Riau', 'Sumatera Utara'], size=10)

# New Cases
new_cases = np.random.randint(50, 200, size=10)

# New Deaths
new_deaths = np.random.randint(1, 10, size=10)

# New Recovered
new_recovered = np.random.randint(20, 100, size=10)

# New Cases
# #new_cases = np.random.randint(50, 200, size=10)
# new_case=int(input("Enter current cases: "))
# new_cases = np.random.randint(new_case, 200, size=10)

# # New Deaths
# new_death=int(input("Enter current deaths: "))
# new_deaths = np.random.randint(new_death, 150, size=10)

# # New Recovered
# new_recover=int(input("Enter current recoveries: "))
# new_recovered = np.random.randint(new_recover, 100, size=10)

# New Active Cases
new_active_cases = new_cases - new_recovered - new_deaths

# Total Cases (cumulative sum of new cases)
total_cases = np.cumsum(new_cases)

# Total Deaths (cumulative sum of new deaths)
total_deaths = np.cumsum(new_deaths)

# Total Recovered (cumulative sum of new recovered)
total_recovered = np.cumsum(new_recovered)

# Total Active Cases
total_active_cases = total_cases - total_recovered - total_deaths

# Other features can also be randomly generated similarly

# Location Level
location_levels = np.random.randint(1, 3, size=10)

# City or Regency
cities_or_regencies = np.random.randint(1, 100, size=10)

# Province
provinces = np.random.choice(['DKI Jakarta', 'Sulawesi Utara', 'Riau', 'Sumatera Utara'], size=10)

# Country
countries = np.random.choice(['Indonesia', 'Other'], size=10)

# Continent
continents = np.random.choice(['Asia', 'Europe', 'North America', 'South America', 'Africa', 'Oceania'], size=10)

# Island
islands = np.random.choice(['Java', 'Sumatra', 'Borneo', 'Sulawesi', 'New Guinea', 'Kalimantan'], size=10)

# Time Zone
time_zones = np.random.choice(['WIB', 'WITA', 'WIT'], size=10)

# Special Status
special_statuses = np.random.choice(['Yes', 'No'], size=10)

# Total Regencies
total_regencies = np.random.randint(1, 50, size=10)

# Total Cities
total_cities = np.random.randint(1, 50, size=10)

# Total Districts
total_districts = np.random.randint(1, 50, size=10)

# Total Urban Villages
total_urban_villages = np.random.randint(1, 50, size=10)

# Total Rural Villages
total_rural_villages = np.random.randint(1, 50, size=10)

# Area (km2)
areas_km2 = np.random.randint(1000, 50000, size=10)

# Population
populations = np.random.randint(100000, 10000000, size=10)

# Population Density
population_densities = np.random.uniform(50, 1000, size=10)

# Longitude
longitudes = np.random.uniform(-180, 180, size=10)

# Latitude
latitudes = np.random.uniform(-90, 90, size=10)

# New Cases per Million
new_cases_per_million = np.random.uniform(10, 500, size=10)

# Total Cases per Million
total_cases_per_million = np.random.uniform(100, 5000, size=10)

# New Deaths per Million
new_deaths_per_million = np.random.uniform(1, 100, size=10)

# Total Deaths per Million
total_deaths_per_million = np.random.uniform(10, 500, size=10)

# # Total Deaths per 100,000
# total_deaths_per_100rb = np.random.uniform(1, 100, size=10)

# Case Fatality Rate
case_fatality_rates = np.random.uniform(0.5, 5, size=10)

# Case Recovered Rate
case_recovered_rates = np.random.uniform(80, 100, size=10)

# Growth Factor of New Cases
growth_factors_new_cases = np.random.uniform(0.5, 2, size=10)

# Growth Factor of New Deaths
growth_factors_new_deaths = np.random.uniform(0.5, 2, size=10)

# Create a DataFrame for the generated data
data = pd.DataFrame({
    'Date': dates,
    'Location_ISO_Code': location_iso_codes,
    'Location': locations,
    'New_Cases': new_cases,
    'New_Deaths': new_deaths,
    'New_Recovered': new_recovered,
    'New_Active_Cases': new_active_cases,
    'Total_Cases': total_cases,
    'Total_Deaths': total_deaths,
    'Total_Recovered': total_recovered,
    'Total_Active_Cases': total_active_cases,
    'Location_Level': location_levels,
    'City_or_Regency': cities_or_regencies,
    'Province': provinces,
    'Country': countries,
    'Continent': continents,
    'Island': islands,
    'Time_Zone': time_zones,
    'Special_Status': special_statuses,
    'Total_Regencies': total_regencies,
    'Total_Cities': total_cities,
    'Total_Districts': total_districts,
    'Total_Urban_Villages': total_urban_villages,
    'Total_Rural_Villages': total_rural_villages,
    'Area_(km2)': areas_km2,
    'Population': populations,
    'Population_Density': population_densities,
    'Longitude': longitudes,
    'Latitude': latitudes,
    'New_Cases_per_Million': new_cases_per_million,
    'Total_Cases_per_Million': total_cases_per_million,
    'New_Deaths_per_Million': new_deaths_per_million,
    'Total_Deaths_per_Million': total_deaths_per_million,
    'Case_Fatality_Rate': case_fatality_rates,
    'Case_Recovered_Rate': case_recovered_rates,
    'Growth_Factor_of_New_Cases':growth_factors_new_cases,
    'Growth_Factor_of_New_Deaths': growth_factors_new_deaths
})

data = data.set_index('Location')
data.head()
# # Display the generated data
# print(data)

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Define LabelEncoder
le = LabelEncoder()

# Function to encode object-type columns to numeric values
def FunLabelEncoder(df):
    for c in df.columns:
        if df.dtypes[c] == object:
            le.fit(df[c].astype(str))
            df[c] = le.transform(df[c].astype(str))
    return df

# Apply label encoding to the DataFrame
data_encoded = FunLabelEncoder(data)
data_encoded['Date'] = pd.to_datetime(data_encoded['Date']).astype(int) // 10**9
data_encoded.info()

# Extract features for 6/11/2020
data_encoded['Date'] = pd.to_datetime(data_encoded['Date']).astype(int) // 10**9

# Filter data for 6/11/2020 and encode the date
encoded_date_6_11_2020 = int(pd.to_datetime('11/6/2020').timestamp()) // 10**9

# Filter data for 6/11/2020
features_6_11_2020 = data_encoded[data_encoded['Date'] == encoded_date_6_11_2020]
#print(features_6_11_2020)
# Reshape the features to match the input shape expected by the model
features1_6_11_2020 = features_6_11_2020.drop(columns=['New_Cases'])#.values.reshape(1, -1)
features2_6_11_2020 = features_6_11_2020.drop(columns=['New_Deaths'])#.values.reshape(1, -1)
#print(features1_6_11_2020.shape)
# Predict for the next 10 days
predictions_next_10_days_new_cases = []
predictions_next_10_days_new_deaths = []

for _ in range(10):
    # Predict new cases
    new_cases_prediction = dtcla.predict(features1_6_11_2020)
    predictions_next_10_days_new_cases.append(new_cases_prediction[_])

    # Predict new deaths
    new_deaths_prediction = dtcla1.predict(features2_6_11_2020)  # Assuming the same model is used for deaths
    predictions_next_10_days_new_deaths.append(new_deaths_prediction[_])

    # Update features for the next day
    # Here you need to update the features_6_11_2020 variable with the predicted values
    # and any other relevant features for the next day

    # Example: If you want to update the 'Date' feature to the next day
    # features_6_11_2020[0, 'Date'] += 1

# Create a DataFrame for predictions
date_range = pd.date_range(start='11/6/2020', periods=10, freq='D')
predictions_df = pd.DataFrame({
    'Date': date_range,
    'New_Cases_Predicted': predictions_next_10_days_new_cases,
    'New_Deaths_Predicted': predictions_next_10_days_new_deaths
})

# Display predictions
print(predictions_df)

# Convert DataFrame to JSON
predictions_json = predictions_df.to_json(orient='records', date_format='iso')

print(predictions_json)

predictions_df['Date'] = predictions_df['Date'].astype(str)
json_data = {
    "name": "COVID-19",
    "data": predictions_df.to_dict(orient='records')
}

import json

with open('covid_predictions.json', 'w') as json_file:
    json.dump(json_data, json_file)

with open('predictions.json', 'w') as f:
   f.write(predictions_json)